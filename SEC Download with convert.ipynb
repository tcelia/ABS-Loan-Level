{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#install BeautifulSoup if needed\n",
    "pip install beautifulsoup4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import necessary modules\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from urllib.request import urlopen \n",
    "import urllib\n",
    "import os\n",
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "import xml.etree.ElementTree as ET\n",
    "import pandas as pd\n",
    "import csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://www.codeproject.com/Articles/1227268/Accessing-Financial-Reports-in-the-EDGAR-Database\n",
    "#First figure out the CIK number for the company you are looking for. update accordingly\n",
    "\n",
    "urlstart=\"https://www.sec.gov/cgi-bin/browse-edgar?action=getcompany\"\n",
    "CIK=\"0001765880\"\n",
    "reporttype=\"ABS-EE\"\n",
    "#key is going to be used in BS to find the links to all the links\n",
    "key='Archives/edgar/data'\n",
    "#key1 is going to be what report name you want to filter on\n",
    "key1=\"exh102\"\n",
    "#this is used in a few places to star the link to the sec's site\n",
    "url2=\"https://sec.gov\"\n",
    "\n",
    "#let's add the count=100 to make sure we get all the filings \n",
    "fullurl=urlstart+\"&CIK=\"+CIK+\"&type=\"+reporttype+\"&count=100\"\n",
    "#print(fullurl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Response [200]>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#import the url to make sure it works. Code of 200 means its good to go. Codes that start with 4 or 5 typically mean there was an error\n",
    "page = requests.get(fullurl)\n",
    "page"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get the html using beautiful soup\n",
    "soup = BeautifulSoup(page.content, 'html.parser')\n",
    "#print(soup.prettify())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create a list of urls that contain archives/edgar/data\n",
    "urls=[]\n",
    "html = urlopen(fullurl)\n",
    "soup = BeautifulSoup(html.read(), 'lxml')\n",
    "links = []\n",
    "for link in soup.find_all('a'):\n",
    "    if key in str(link):\n",
    "        urls.append(link.get('href'))\n",
    "#print(urls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from the list of urls created in the last step, visit each and get a list of urls that contain the link to \n",
    "#the actual xml files. filter on 'xh102'\n",
    "urls1=[]\n",
    "for url in urls:\n",
    "    newurl=url2+url\n",
    "    page1 = requests.get(newurl)\n",
    "    soup1 = BeautifulSoup(page1.content, 'html.parser')\n",
    "    for link in soup1.find_all('a'):\n",
    "        if key1 in str(link):\n",
    "            urls1.append(link.get('href'))\n",
    "#print(urls1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "#save the xml files down. this code works. write the loop to save all files down\n",
    "\n",
    "#Create a subfolder in 'SEC' labelled with the CIK#\n",
    "if not os.path.exists('sec/'+CIK):\n",
    "    os.makedirs('sec/'+CIK)\n",
    "#get a list of files in that directory\n",
    "onlyfiles = [os.path.splitext(f)[0] for f in listdir('sec/'+CIK) if isfile(join('sec/'+CIK, f))]\n",
    "\n",
    "#if the filename you are referencing is NOT in the directory already, download\n",
    "for url in urls1:\n",
    "    path=url2+url\n",
    "    filenamefull=path[path.rfind(\"/\")+1:]\n",
    "    filename=os.path.splitext(path[path.rfind(\"/\")+1:])[0]\n",
    "    if filename not in onlyfiles:\n",
    "        urllib.request.urlretrieve(path, \"sec/\"+CIK+\"/\"+filenamefull)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "#xml files are real big. Let's go through the folder and convert the data to CSV files. CSV files are ~5-8x smaller than XML\n",
    "#a function that will convert an XML to a dataframe\n",
    "def parse_XML(xml_file, df_cols): \n",
    "    \"\"\"Parse the input XML file and store the result in a pandas \n",
    "    DataFrame with the given columns. \n",
    "    \"\"\"\n",
    "    \n",
    "    xtree = ET.parse(xml_file)\n",
    "    xroot = xtree.getroot()\n",
    "    rows = []\n",
    "    \n",
    "    for node in xroot: \n",
    "        res = []\n",
    "        for el in df_cols[0:]: \n",
    "            if node is not None and node.find(el, namespaces) is not None:\n",
    "                res.append(node.find(el, namespaces).text)\n",
    "            else: \n",
    "                res.append(None)\n",
    "        rows.append({df_cols[i]: res[i] \n",
    "                     for i, _ in enumerate(df_cols)})\n",
    "    \n",
    "    out_df = pd.DataFrame(rows, columns=df_cols)\n",
    "        \n",
    "    return out_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns=[]\n",
    "with open(\"ABS EE Columns.txt\", 'r') as inputfile:\n",
    "    columns=inputfile.read().split(\",\")\n",
    "#print(columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['exh1025940092020.xml']\n"
     ]
    }
   ],
   "source": [
    "#get a list of xml files to convert\n",
    "xmlfiles=[file for file in os.listdir('sec/'+CIK) if file.endswith(\".xml\")]\n",
    "print(xmlfiles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "#go through each file, turn into dataframe, export to CSV, delete XML file\n",
    "for file in xmlfiles:\n",
    "    #Uses a list comprehension and element tree's iterparse function to create a dictionary containing the namespace prefix and it's uri\n",
    "    #The underscore is utilized to remove the \"start-ns\" output from the list.\n",
    "    namespaces = {node[0]: node[1] for _, node in ET.iterparse('sec/'+CIK+'/'+file, events=['start-ns'])}\n",
    "#Iterates through the newly created namespace list registering each one.\n",
    "    for key, value in namespaces.items():\n",
    "        ET.register_namespace(key, value)\n",
    "    #print(namespaces)\n",
    "    a=parse_XML('sec/'+CIK+'/'+file,columns)\n",
    "    a.to_csv('sec/'+CIK+'/'+file.rstrip(\".xml\")+'.csv', index=False)\n",
    "    os.remove('sec/'+CIK+'/'+file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
